<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Sign2Text</title>
<style>
  body {
    margin: 0;
    background: #121212;
    font-family: 'Segoe UI', Roboto, sans-serif;
    display: flex;
    flex-direction: column;
    align-items: center;
    color: #00ffd0;
  }

  h1 {
    margin: 20px 0;
    font-size: 1.8rem;
    text-align: center;
  }

  #videoWrap {
    position: relative;
    width: 90vw;
    max-width: 640px;
    aspect-ratio: 4/3;
    border-radius: 16px;
    overflow: hidden;
    box-shadow: 0 8px 20px rgba(0,0,0,0.5);
    background: #222;
  }

  video, canvas {
    position: absolute;
    top: 0; left: 0;
    width: 100%;
    height: 100%;
    object-fit: cover;
    transform: scaleX(-1);
  }

  #word {
    margin-top: 15px;
    font-size: 2rem;
    font-weight: bold;
    text-shadow: 0 0 10px #000;
    min-height: 2.5rem;
    text-align: center;
  }
</style>
</head>
<body>

<h1>Sign2Text</h1>

<div id="videoWrap">
  <video id="webcam" autoplay playsinline muted></video>
  <canvas id="overlay"></canvas>
</div>

<div id="word">â€”</div>

<!-- TensorFlow.js + Fingerpose -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose@0.0.7/dist/handpose.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>
<script src="./handposeutil.js"></script>
<script src="./handsigns.js"></script>

<script>
const video = document.getElementById('webcam');
const overlay = document.getElementById('overlay');
const ctx = overlay.getContext('2d');
const wordEl = document.getElementById('word');

let model = null;
let constructedWord = '';
let stableLetter = '';
let stableCount = 0;
const ACCEPT_THRESHOLD = 3;
const INTERVAL_MS = 150;

async function startCamera(){
  const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
  video.srcObject = stream;
  await video.play();
  overlay.width = video.videoWidth;
  overlay.height = video.videoHeight;
}

function draw(predictions){
  overlay.width = video.videoWidth;
  overlay.height = video.videoHeight;
  ctx.clearRect(0,0,overlay.width,overlay.height);
  if(window.drawHand && predictions?.length) drawHand(predictions, ctx);
}

function sendToAppInventor(payload){
  try{
    if(window.AppInventor?.setWebViewString) AppInventor.setWebViewString(payload);
    else if(window.parent?.AppInventor?.setWebViewString) window.parent.AppInventor.setWebViewString(payload);
  }catch{}
}

function speakText(text){
  if(!text) return;
  const u = new SpeechSynthesisUtterance(text);
  window.speechSynthesis.cancel();
  window.speechSynthesis.speak(u);
}

async function loadModel(){ model = await handpose.load(); }

function acceptLetter(letter){
  constructedWord += letter;
  wordEl.textContent = constructedWord || 'â€”';
  speakText(letter);
  sendToAppInventor('WORD:' + constructedWord);
}

async function analyzeFrame(){
  if(!model || !video.videoWidth) return;
  const predictions = await model.estimateHands(video);
  draw(predictions);

  if(predictions?.length && window.fp && window.Handsigns){
    const GE = new fp.GestureEstimator(Object.values(Handsigns));
    const est = await GE.estimate(predictions[0].landmarks, 7.5);
    if(est.gestures?.length){
      const max = est.gestures.reduce((a,b)=>a.confidence>b.confidence?a:b);
      let letter = max.name?.[0]?.toUpperCase() || '';
      if(letter){
        if(letter === stableLetter) stableCount++;
        else { stableLetter = letter; stableCount = 1; }
        if(stableCount >= ACCEPT_THRESHOLD){ acceptLetter(letter); stableLetter=''; stableCount=0; }
      } else { stableLetter=''; stableCount=0; }
    } else { stableLetter=''; stableCount=0; }
  } else { stableLetter=''; stableCount=0; }
}

// --- Auto-start ---
(async ()=>{
  await startCamera();
  await loadModel();
  setInterval(analyzeFrame, INTERVAL_MS);
})();
</script>

</body>
</html>

  <h1>ASL Sign â†’ Word</h1>

  <div id="videoWrap">
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div id="word">â€”</div>

  <div id="controls">
    <button id="start">Start</button>
    <button id="stop">Stop</button>
    <button id="clear">Clear Word</button>
    <button id="speak">Speak Word</button>
  </div>

  <div id="notice">
    Make letters with your dominant hand. The page will speak each accepted letter and update the full word. It will also send <code>WORD:&lt;yourword&gt;</code> to App Inventor via <code>AppInventor.setWebViewString</code>.
  </div>

  <!-- TensorFlow.js + Fingerpose -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose@0.0.7/dist/handpose.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>
  <script src="./handposeutil.js"></script>
  <script src="./handsigns.js"></script>

  <script>
  const video = document.getElementById('webcam');
  const overlay = document.getElementById('overlay');
  const ctx = overlay.getContext('2d');
  const wordEl = document.getElementById('word');
  const startBtn = document.getElementById('start');
  const stopBtn = document.getElementById('stop');
  const clearBtn = document.getElementById('clear');
  const speakBtn = document.getElementById('speak');

  let model = null;
  let runInterval = null;
  let running = false;
  let constructedWord = '';
  let stableLetter = '';
  let stableCount = 0;
  const ACCEPT_THRESHOLD = 3;
  const INTERVAL_MS = 150;

  async function startCamera(){
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
      video.srcObject = stream;
      await video.play();
      overlay.width = video.videoWidth;
      overlay.height = video.videoHeight;
    } catch(e) { alert('Camera error: ' + e.message); }
  }

  async function stopCamera(){
    if(video.srcObject) video.srcObject.getTracks().forEach(t=>t.stop());
    video.pause(); video.srcObject = null;
  }

  function draw(predictions){
    overlay.width = video.videoWidth;
    overlay.height = video.videoHeight;
    ctx.clearRect(0,0,overlay.width,overlay.height);
    if(window.drawHand && predictions?.length) drawHand(predictions, ctx);
  }

  function sendToAppInventor(payload){
    try {
      if(window.AppInventor?.setWebViewString) AppInventor.setWebViewString(payload);
      else if(window.parent?.AppInventor?.setWebViewString) window.parent.AppInventor.setWebViewString(payload);
    } catch {}
  }

  function speakText(text){
    if(!text) return;
    const u = new SpeechSynthesisUtterance(text);
    window.speechSynthesis.cancel(); // cancel queued speech
    window.speechSynthesis.speak(u);
  }

  async function loadModel(){ model = await handpose.load(); }

  function acceptLetter(letter){
    constructedWord += letter;
    wordEl.textContent = constructedWord || 'â€”';
    speakText(letter);
    sendToAppInventor('WORD:' + constructedWord);
  }

  async function analyzeFrame(){
    if(!model || !video.videoWidth) return;
    const predictions = await model.estimateHands(video);
    draw(predictions);
    if(predictions?.length && window.fp && window.Handsigns){
      const GE = new fp.GestureEstimator(Object.values(Handsigns));
      const est = await GE.estimate(predictions[0].landmarks, 7.5);
      if(est.gestures?.length){
        const max = est.gestures.reduce((a,b)=>a.confidence>b.confidence?a:b);
        let letter = max.name?.[0]?.toUpperCase() || '';
        if(letter){
          if(letter === stableLetter) stableCount++;
          else { stableLetter = letter; stableCount = 1; }
          if(stableCount >= ACCEPT_THRESHOLD){ acceptLetter(letter); stableLetter=''; stableCount=0; }
        } else { stableLetter=''; stableCount=0; }
      } else { stableLetter=''; stableCount=0; }
    } else { stableLetter=''; stableCount=0; }
  }

  // --- Button handlers ---
  startBtn.addEventListener('click', async ()=>{
    if(running) return;
    await startCamera();
    if(!model) await loadModel();
    runInterval = setInterval(analyzeFrame, INTERVAL_MS);
    running = true;
  });

  stopBtn.addEventListener('click', ()=>{
    if(!running) return;
    clearInterval(runInterval); runInterval=null; running=false;
    stopCamera();
  });

  clearBtn.addEventListener('click', ()=>{
    constructedWord=''; wordEl.textContent='â€”';
    sendToAppInventor('WORD:' + constructedWord);
  });

  speakBtn.addEventListener('click', ()=> speakText(constructedWord));
  </script>

</body>
</html>
  <div id="videoWrap">
    <video id="webcam" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
    <div id="word">Show a sign...</div>
    <button id="muteBtn">ðŸ”Š</button>
  </div>

  <!-- TensorFlow.js and Fingerpose -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose@0.0.7/dist/handpose.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>

  <!-- Helper scripts (must exist beside this file) -->
  <script src="./handposeutil.js"></script>
  <script src="./handsigns.js"></script>

  <script>
  const video = document.getElementById('webcam');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');
  const wordEl = document.getElementById('word');
  const muteBtn = document.getElementById('muteBtn');

  let model = null;
  let interval = null;
  let running = false;
  let constructedWord = '';
  let stableLetter = '';
  let stableCount = 0;
  const ACCEPT_THRESHOLD = 3;
  const INTERVAL_MS = 150;
  let speakEnabled = true;

  // --- Camera ---
  async function startCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
    video.srcObject = stream;
    await video.play();
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
  }

  // --- Speak ---
  function speak(text) {
    if (!speakEnabled) return;
    try {
      const u = new SpeechSynthesisUtterance(text);
      speechSynthesis.speak(u);
    } catch {}
  }

  // --- Send to App Inventor ---
  function sendToAppInventor(payload) {
    try {
      if (window.AppInventor?.setWebViewString) {
        window.AppInventor.setWebViewString(payload);
      } else if (window.parent?.AppInventor?.setWebViewString) {
        window.parent.AppInventor.setWebViewString(payload);
      }
    } catch {}
  }

  // --- Accept letter ---
  function acceptLetter(letter) {
    constructedWord += letter;
    wordEl.textContent = constructedWord;
    speak(letter);
    sendToAppInventor('WORD:' + constructedWord);
  }

  // --- Main detection ---
  async function analyzeFrame() {
    if (!model || !video.videoWidth) return;
    const hands = await model.estimateHands(video);
    if (window.drawHand) drawHand(hands, ctx);
    if (hands.length === 0) {
      stableLetter = ''; stableCount = 0; return;
    }

    const GE = new fp.GestureEstimator(Object.values(window.Handsigns));
    const est = await GE.estimate(hands[0].landmarks, 7.5);
    if (!est.gestures?.length) { stableLetter=''; stableCount=0; return; }

    const max = est.gestures.reduce((a,b)=>a.confidence>b.confidence?a:b);
    let letter = max.name?.[0]?.toUpperCase() || '';
    if (!letter) return;

    if (letter === stableLetter) {
      stableCount++;
      if (stableCount >= ACCEPT_THRESHOLD) {
        acceptLetter(letter);
        stableLetter=''; stableCount=0;
      }
    } else {
      stableLetter = letter;
      stableCount = 1;
    }
  }

  // --- Toggle sound ---
  muteBtn.addEventListener('click', ()=>{
    speakEnabled = !speakEnabled;
    muteBtn.textContent = speakEnabled ? 'ðŸ”Š' : 'ðŸ”‡';
  });

  // --- Load model and start ---
  (async()=>{
    await startCamera();
    model = await handpose.load();
    interval = setInterval(analyzeFrame, INTERVAL_MS);
    running = true;
  })();
  </script>
</body>
</html>
  <div id="container">
    <h2 style="text-align:center">ASL Sign â†’ Word (Aâ€“Z) â€” App Inventor Bridge</h2>

    <div id="videoWrap">
      <video id="webcam" autoplay playsinline muted></video>
      <canvas id="overlay"></canvas>
    </div>

    <div id="word">â€”</div>

    <div id="controls">
      <button id="start">Start</button>
      <button id="stop">Stop</button>
      <button id="clear">Clear Word</button>
      <button id="speak">Speak Word</button>
    </div>

    <div id="notice">
      Make letters with your dominant hand. Page will speak each accepted letter and update the full word. The page will also send `WORD:<yourword>` to App Inventor via `AppInventor.setWebViewString`.
    </div>
  </div>

  <!-- TensorFlow.js and fingerpose -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose@0.0.7/dist/handpose.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>

  <!-- Helper files from the Handsign repo.
       You must place handposeutil.js and handsigns.js in the same folder as this index.html.
       I list the exact URLs below in 'Where to get helper files'. -->
  <script src="./handposeutil.js"></script>
  <script src="./handsigns.js"></script>

  <script>
  // Minimal signâ†’word page using Handpose + Fingerpose + Handsigns helper definitions.
  // Requirements: handposeutil.js and handsigns.js (from syauqy/handsign-tensorflow repo) must exist next to this index.html.

  const video = document.getElementById('webcam');
  const overlay = document.getElementById('overlay');
  const ctx = overlay.getContext('2d');
  const wordEl = document.getElementById('word');
  const startBtn = document.getElementById('start');
  const stopBtn = document.getElementById('stop');
  const clearBtn = document.getElementById('clear');
  const speakBtn = document.getElementById('speak');

  let model = null;
  let runInterval = null;
  let running = false;
  let constructedWord = '';
  let stableLetter = '';
  let stableCount = 0;
  const ACCEPT_THRESHOLD = 3; // require same letter repeated this many times
  const INTERVAL_MS = 150;

  // Draw wrapper uses drawHand from handposeutil.js (from Handsign repo)
  function draw(hand) {
    overlay.width = video.videoWidth;
    overlay.height = video.videoHeight;
    ctx.clearRect(0,0,overlay.width,overlay.height);
    if (window.drawHand && hand) {
      drawHand(hand, ctx); // provided by handposeutil.js
    }
  }

  async function startCamera(){
    try{
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio:false });
      video.srcObject = stream;
      await video.play();
      overlay.width = video.videoWidth;
      overlay.height = video.videoHeight;
    }catch(e){
      alert('Camera error: ' + e.message + '. Make sure you allow camera access and open page over HTTPS.');
      return;
    }
  }

  async function stopCamera(){
    try{
      const s = video.srcObject;
      if (s) s.getTracks().forEach(t=>t.stop());
      video.pause();
      video.srcObject = null;
    }catch(e){}
  }

  function sendToAppInventor(payload){
    try{
      if (window.AppInventor && AppInventor.setWebViewString) {
        AppInventor.setWebViewString(payload);
      } else if (window.parent && window.parent.AppInventor && window.parent.AppInventor.setWebViewString) {
        window.parent.AppInventor.setWebViewString(payload);
      } else {
        // not embedded â€” fine for testing in browser
        //console.log('AppInventor payload:', payload);
      }
    }catch(e){ console.warn(e); }
  }

  function speakText(t){
    try{
      const u = new SpeechSynthesisUtterance(t);
      window.speechSynthesis.speak(u);
    }catch(e){}
  }

  async function loadModel(){
    // handpose is loaded as window.handpose when we included the CDN script above.
    model = await handpose.load();
    console.log('handpose loaded', model);
  }

  function acceptLetter(letter){
    // append to constructed word
    constructedWord += letter;
    wordEl.innerText = constructedWord || 'â€”';
    // speak the letter
    speakText(letter);
    // send full word to App Inventor
    sendToAppInventor('WORD:' + constructedWord);
  }

  // Main detection loop
  async function analyzeFrame(){
    if (!model) return;
    if (!video || !video.videoWidth) return;
    const predictions = await model.estimateHands(video);
    draw(predictions);
    if (predictions && predictions.length > 0) {
      // create Fingerpose estimator with the finger gestures loaded from handsigns.js
      // handsigns.js should define an object "Handsigns" with properties like aSign, bSign, ... (same names used in Handsign repo)
      if (window.fp && window.Handsigns) {
        const GE = new fp.GestureEstimator([
          fp.Gestures.ThumbsUpGesture,
          Handsigns.aSign,
          Handsigns.bSign,
          Handsigns.cSign,
          Handsigns.dSign,
          Handsigns.eSign,
          Handsigns.fSign,
          Handsigns.gSign,
          Handsigns.hSign,
          Handsigns.iSign,
          Handsigns.jSign,
          Handsigns.kSign,
          Handsigns.lSign,
          Handsigns.mSign,
          Handsigns.nSign,
          Handsigns.oSign,
          Handsigns.pSign,
          Handsigns.qSign,
          Handsigns.rSign,
          Handsigns.sSign,
          Handsigns.tSign,
          Handsigns.uSign,
          Handsigns.vSign,
          Handsigns.wSign,
          Handsigns.xSign,
          Handsigns.ySign,
          Handsigns.zSign,
        ]);

        const est = await GE.estimate(predictions[0].landmarks, 7.5);
        if (est && est.gestures && est.gestures.length > 0) {
          const confidences = est.gestures.map(g=>g.confidence);
          const idx = confidences.indexOf(Math.max(...confidences));
          const name = est.gestures[idx].name || '';
          // normalize to letter
          let letter = '';
          if (typeof name === 'string') {
            const m = name.match(/[a-zA-Z]/);
            if (m) letter = m[0].toUpperCase();
          }
          if (letter) {
            if (stableLetter === letter) {
              stableCount++;
            } else {
              stableLetter = letter;
              stableCount = 1;
            }
            if (stableCount >= ACCEPT_THRESHOLD) {
              acceptLetter(letter);
              stableLetter = '';
              stableCount = 0;
            }
          } else {
            stableLetter = '';
            stableCount = 0;
          }
        } else {
          stableLetter = '';
          stableCount = 0;
        }
      } else {
        // handsigns.js or fingerpose not found â€” notify user
        console.warn('fingerpose or Handsigns not available. Make sure handsigns.js and fingerpose are loaded.');
      }
    } else {
      // no hand
      stableLetter = '';
      stableCount = 0;
    }
  }

  // Control handlers
  startBtn.addEventListener('click', async ()=>{
    if (running) return;
    await startCamera();
    if (!model) await loadModel();
    runInterval = setInterval(analyzeFrame, INTERVAL_MS);
    running = true;
  });

  stopBtn.addEventListener('click', async ()=>{
    if (!running) return;
    clearInterval(runInterval);
    runInterval = null;
    running = false;
    await stopCamera();
  });

  clearBtn.addEventListener('click', ()=>{
    constructedWord = '';
    wordEl.innerText = 'â€”';
    sendToAppInventor('WORD:' + constructedWord);
  });

  speakBtn.addEventListener('click', ()=>{
    speakText(constructedWord || '');
  });

  // Auto-start if page is opened directly and user interacts (some browsers require user interaction)
  // You can call startBtn.click() manually to start.
  </script>
</body>
</html>

